{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import model_selection, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/abdkumar/Google-Analytics-Customer-Revenue-Prediction/blob/master/customer%20revenue%20prediction.ipynb\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "gc.enable()\n",
    "features = ['channelGrouping', 'date', 'fullVisitorId', 'visitId',\\\n",
    "       'visitNumber', 'visitStartTime', 'device.browser',\\\n",
    "       'device.deviceCategory', 'device.isMobile', 'device.operatingSystem',\\\n",
    "       'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country',\\\n",
    "       'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region',\\\n",
    "       'geoNetwork.subContinent', 'totals.bounces', 'totals.hits',\\\n",
    "       'totals.newVisits', 'totals.pageviews', 'totals.transactionRevenue',\\\n",
    "       'trafficSource.adContent', 'trafficSource.campaign',\\\n",
    "       'trafficSource.isTrueDirect', 'trafficSource.keyword',\\\n",
    "       'trafficSource.medium', 'trafficSource.referralPath',\\\n",
    "       'trafficSource.source']\n",
    "def load_df(csv_path):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    ans = pd.DataFrame()\n",
    "    dfs = pd.read_csv(csv_path, sep=',',\n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                    chunksize = 50000)\n",
    "    for df in dfs:\n",
    "        df.reset_index(drop = True,inplace = True)\n",
    "        for column in JSON_COLUMNS:\n",
    "            column_as_df = json_normalize(df[column])\n",
    "            column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "            df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "        print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "        use_df = df[features]\n",
    "        del df\n",
    "        gc.collect()\n",
    "        ans = pd.concat([ans, use_df], axis = 0).reset_index(drop = True)\n",
    "        print(ans.shape)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"E:\\\\data science case studys\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = load_df(path+\"train_v2.csv\")\n",
    "test_df = load_df(path+\"test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not usefull columns\n",
    "col_drop=[\"totals.bounces\",\"trafficSource.referralPath\",\"trafficSource.keyword\",\"trafficSource.campaign\"]\n",
    "train=train_df.drop(col_drop,axis=1)\n",
    "test=test_df.drop(col_drop,axis=1)\n",
    "train_test_data = pd.concat([train,test], axis=0).reset_index()#concat the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the data types\n",
    "train_test_data['totals.hits'] = train_test_data['totals.hits'].astype(float)\n",
    "train_test_data['totals.pageviews'] = train_test_data['totals.pageviews'].astype(float)\n",
    "train_test_data['totals.newVisits'] = train_test_data['totals.newVisits'].astype(float)\n",
    "train_test_data['device.isMobile'] = train_test_data['device.isMobile'].astype(bool)\n",
    "train_test_data['trafficSource.isTrueDirect'] = train_test_data['trafficSource.isTrueDirect'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the missing values in transactionrevenue columns\n",
    "train_test_data['totals.transactionRevenue'] = train_test_data['totals.transactionRevenue'].astype(float)\n",
    "train_test_data['totals.transactionRevenue'].fillna(0, inplace=True)\n",
    "target = train_test_data['totals.transactionRevenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/HuanZhang999/GoogleAnalyticsCustomerRevenuePrediction/blob/master/1-%20create_train.ipynb\n",
    "def getTimeFramewithFeatures(tr, k=1):\n",
    "    \n",
    "    tf = train_test_data.loc[(train_test_data['date'] >= min(train_test_data['date']) + timedelta(days=168*(k-1))) \n",
    "              & (train_test_data['date'] < min(train_test_data['date']) + timedelta(days=168*k))]\n",
    "\n",
    "    tf_fvid = set(train_test_data.loc[(train_test_data['date'] >= min(train_test_data['date']) + timedelta(days=168*k + 46 )) \n",
    "                       & (train_test_data['date'] < min(train_test_data['date']) + timedelta(days=168*k + 46 + 62))]['fullVisitorId'])\n",
    "\n",
    "    tf_returned = tf[tf['fullVisitorId'].isin(tf_fvid)]\n",
    "    \n",
    "    tf_tst = train_test_data[train_test_data['fullVisitorId'].isin(set(tf_returned['fullVisitorId']))\n",
    "             & (train_test_data['date'] >= min(train_test_data['date']) + timedelta(days=168*k + 46))\n",
    "             & (train_test_data['date'] < min(train_test_data['date']) + timedelta(days=168*k + 46 + 62))]\n",
    "    \n",
    "    \n",
    "    tf_target = tf_tst.groupby('fullVisitorId')[['totals.transactionRevenue']].sum().apply(np.log1p, axis=1).reset_index()\n",
    "    tf_target['ret'] = 1\n",
    "    tf_target.rename(columns={'totals.transactionRevenue': 'target'}, inplace=True)\n",
    "    \n",
    "    tf_nonret = pd.DataFrame()\n",
    "    tf_nonret['fullVisitorId'] = list(set(tf['fullVisitorId']) - tf_fvid)    \n",
    "    tf_nonret['target'] = 0\n",
    "    tf_nonret['ret'] = 0\n",
    "    \n",
    "    tf_target = pd.concat([tf_target, tf_nonret], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    tf_maxdate = max(tf['date'])\n",
    "    tf_mindate = min(tf['date'])\n",
    "\n",
    "    tf = tf.groupby('fullVisitorId').agg({\n",
    "            'geoNetwork.networkDomain': {'networkDomain': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.city': {'city': lambda x: x.dropna().max()},\n",
    "            'device.operatingSystem': {'operatingSystem': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.metro': {'metro': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.region': {'region': lambda x: x.dropna().max()},\n",
    "            'channelGrouping': {'channelGrouping': lambda x: x.dropna().max()},\n",
    "    \n",
    "            'geoNetwork.country': {'country': lambda x: x.dropna().max()},\n",
    "            'trafficSource.source': {'source': lambda x: x.dropna().max()},\n",
    "            'trafficSource.medium': {'medium': lambda x: x.dropna().max()},\n",
    "            'device.browser':  {'browser': lambda x: x.dropna().max()},\n",
    "            'device.deviceCategory': {'deviceCategory': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.continent': {'continent': lambda x: x.dropna().max()},\n",
    "            'totals.pageviews': {'pageviews_sum': lambda x: x.dropna().sum(),\n",
    "                                 'pageviews_min': lambda x: x.dropna().min(), \n",
    "                                 'pageviews_max': lambda x: x.dropna().max(),\n",
    "                                 'pageviews_mean': lambda x: x.dropna().mean()},\n",
    "            'totals.hits': {'hits_sum': lambda x: x.dropna().sum(), \n",
    "                            'hits_min': lambda x: x.dropna().min(), \n",
    "                            'hits_max': lambda x: x.dropna().max(), \n",
    "                            'hits_mean': lambda x: x.dropna().mean()},\n",
    "    \n",
    "            'visitStartTime': {'visitStartTime_counts': lambda x: x.dropna().count()},\n",
    "            'trafficSource.isTrueDirect': {'isTrueDirect': lambda x: x.dropna().max()},\n",
    "            'totals.newVisits': {'newVisits_max': lambda x: x.dropna().max()},\n",
    "            'device.isMobile': {'isMobile': lambda x: x.dropna().max()},\n",
    "            'visitNumber': {'visitNumber_max' : lambda x: x.dropna().max()},\n",
    "    \n",
    "            'totals.transactionRevenue':  {'transactionRevenue_sum':  lambda x:x.dropna().sum()},\n",
    "            \n",
    "            'date': {'first_ses_from_the_period_start': lambda x: x.dropna().min() - tf_mindate,\n",
    "                     \n",
    "                     'last_ses_from_the_period_end': lambda x: tf_maxdate - x.dropna().max(),\n",
    "                     \n",
    "                     'interval_dates': lambda x: x.dropna().max() - x.dropna().min(),\n",
    "                     \n",
    "                     'unqiue_date_num': lambda x: len(set(x.dropna())) },            \n",
    "                    })\n",
    "\n",
    "    tf.columns = tf.columns.droplevel()\n",
    "\n",
    "    tf = pd.merge(tf, tf_target, left_on='fullVisitorId', right_on='fullVisitorId')\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get 1st train part...')\n",
    "tr1 = getTimeFramewithFeatures(train_test_data, k=1)\n",
    "tr1.to_pickle(path+\"new_tr1_clean\")#save the data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get 1st train part...')\n",
    "tr2 = getTimeFramewithFeatures(train_test_data, k=2)\n",
    "tr2.to_pickle(path+\"new_tr2_clean\")#save the data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get 1st train part...')\n",
    "tr3 = getTimeFramewithFeatures(train_test_data, k=3)\n",
    "tr3.to_pickle(path+\"new_tr3_clean\")#save the data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get 1st train part...')\n",
    "tr4 = getTimeFramewithFeatures(train_test_data, k=4)\n",
    "tr4.to_pickle(path+\"new_tr4_clean\")#save the data into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construction of the test-set (by analogy as train-set)\n",
    "print('Get test')\n",
    "tr5 = train_test_data[train_test_data['date'] >= pd.to_datetime(20180501, infer_datetime_format=True, format=\"%Y%m%d\")]\n",
    "tr5_maxdate = max(tr5['date'])\n",
    "tr5_mindate = min(tr5['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tr5 = tr5.groupby('fullVisitorId').agg({\n",
    "            'geoNetwork.networkDomain': {'networkDomain': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.city': {'city': lambda x: x.dropna().max()},\n",
    "            'device.operatingSystem': {'operatingSystem': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.metro': {'metro': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.region': {'region': lambda x: x.dropna().max()},\n",
    "            'channelGrouping': {'channelGrouping': lambda x: x.dropna().max()},\n",
    "    \n",
    "            'geoNetwork.country': {'country': lambda x: x.dropna().max()},\n",
    "            'trafficSource.source': {'source': lambda x: x.dropna().max()},\n",
    "            'trafficSource.medium': {'medium': lambda x: x.dropna().max()},\n",
    "            'device.browser':  {'browser': lambda x: x.dropna().max()},\n",
    "            'device.deviceCategory': {'deviceCategory': lambda x: x.dropna().max()},\n",
    "            'geoNetwork.continent': {'continent': lambda x: x.dropna().max()},\n",
    "            'totals.pageviews': {'pageviews_sum': lambda x: x.dropna().sum(),\n",
    "                                 'pageviews_min': lambda x: x.dropna().min(), \n",
    "                                 'pageviews_max': lambda x: x.dropna().max(),\n",
    "                                 'pageviews_mean': lambda x: x.dropna().mean()},\n",
    "            'totals.hits': {'hits_sum': lambda x: x.dropna().sum(), \n",
    "                            'hits_min': lambda x: x.dropna().min(), \n",
    "                            'hits_max': lambda x: x.dropna().max(), \n",
    "                            'hits_mean': lambda x: x.dropna().mean()},\n",
    "    \n",
    "            'visitStartTime': {'visitStartTime_counts': lambda x: x.dropna().count()},\n",
    "            'trafficSource.isTrueDirect': {'isTrueDirect': lambda x: x.dropna().max()},\n",
    "            'totals.newVisits': {'newVisits_max': lambda x: x.dropna().max()},\n",
    "            'device.isMobile': {'isMobile': lambda x: x.dropna().max()},\n",
    "            'visitNumber': {'visitNumber_max' : lambda x: x.dropna().max()},\n",
    "    \n",
    "            'totals.transactionRevenue':  {'transactionRevenue_sum':  lambda x:x.dropna().sum()},\n",
    "            \n",
    "            'date': {'first_ses_from_the_period_start': lambda x: x.dropna().min() - tf_mindate,\n",
    "                     \n",
    "                     'last_ses_from_the_period_end': lambda x: tf_maxdate - x.dropna().max(),\n",
    "                     \n",
    "                     'interval_dates': lambda x: x.dropna().max() - x.dropna().min(),\n",
    "                     \n",
    "                     'unqiue_date_num': lambda x: len(set(x.dropna())) },            \n",
    "                    })\n",
    "tr5.columns = tr5.columns.droplevel()\n",
    "tr5['target'] = np.nan\n",
    "tr5['ret'] = np.nan'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr5.shape\n",
    "tr5.to_pickle(path+\"new_tr5_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_tr1 = pd.read_pickle(path+\"new_tr1_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr2 = pd.read_pickle(path+\"new_tr2_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr3 = pd.read_pickle(path+\"new_tr3_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr4 = pd.read_pickle(path+\"new_tr4_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr5 = pd.read_pickle(path+\"new_tr5_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat all the train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([new_tr1,new_tr2,new_tr3,new_tr4,new_tr5], axis=0, sort=False).reset_index(drop=True)\n",
    "train_all['interval_dates'] = train_all['interval_dates'].dt.days#fatch the day form date \n",
    "train_all['first_ses_from_the_period_start'] = train_all['first_ses_from_the_period_start'].dt.days\n",
    "train_all['last_ses_from_the_period_end'] = train_all['last_ses_from_the_period_end'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1819164, 61)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullVisitorId_new=new_tr5.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(s):\n",
    "    if s == 'True':\n",
    "         return True\n",
    "    elif s == 'False':\n",
    "         return False\n",
    "    else:\n",
    "         return False  #ValueError # evil ValueError that doesn't tell you what the wrong value was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all[\"isTrueDirect\"]=train_all[\"isTrueDirect\"].apply(lambda x : str_to_bool(x))\n",
    "train_all[\"isMobile\"]=train_all[\"isMobile\"].apply(lambda x : str_to_bool(x))\n",
    "train_all[\"device.isMobile\"]=train_all[\"device.isMobile\"].apply(lambda x : str_to_bool(x))\n",
    "train_all[\"trafficSource.isTrueDirect\"]=train_all[\"trafficSource.isTrueDirect\"].apply(lambda x : str_to_bool(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "def final_fun_1(train_all):\n",
    "    #train_all.isnull().sum()\n",
    "    train_data_y = train_all['target']#store the targets \n",
    "    train_data_ret = train_all['ret']\n",
    "    #-----------------\n",
    "    not_use=[\"target\",\"ret\",\"date\"]\n",
    "    #fatch only categorical data\n",
    "    categorical_cols = list()\n",
    "    for i in train_all.columns:\n",
    "        if (train_all[i].dtype=='object' or train_all[i].dtype=='bool') :\n",
    "            categorical_cols.append(i)\n",
    "      #categorical_cols.remove(\"fullVisitorId\")\n",
    "    categorical_cols.remove(\"isTrueDirect\")\n",
    "    categorical_cols.remove(\"isMobile\")\n",
    "    categorical_cols.remove(\"device.isMobile\")\n",
    "    categorical_cols.remove(\"trafficSource.isTrueDirect\")        \n",
    "    #fatch only numerical data\n",
    "    num_cols = list()\n",
    "    for i in train_all.columns:\n",
    "        if train_all[i].dtype not in ['object', 'bool']:\n",
    "            num_cols.append(i)\n",
    "    #num_cols        \n",
    "    numerical_columns = [c for c in num_cols if c not in not_use]\n",
    "    #-------------------------------\n",
    "    # fillmissing \n",
    "    for col in categorical_cols:\n",
    "        train_all[col].fillna('missing', inplace=True) \n",
    "\n",
    "    for col in numerical_columns:       \n",
    "        train_all[col].fillna(0, inplace=True)\n",
    "    #------------------------------------\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # Fit your data on the scaler object\n",
    "    scaled_df = scaler.fit_transform(train_all[numerical_columns])\n",
    "    scaled_df = pd.DataFrame(scaled_df, columns=numerical_columns)\n",
    "    train_all[numerical_columns] = scaled_df\n",
    "    #------------------------------------------\n",
    "    train = train_all[train_all['target'].notnull()]\n",
    "    test = train_all[train_all['target'].isnull()]\n",
    "    #------------------------------------------\n",
    "    for col in categorical_cols:\n",
    "    # Using whole data to form an exhaustive list of levels\n",
    "        data=train[col].append(test[col])\n",
    "        le.fit(data.values)\n",
    "        train[col]=le.transform(train[col])\n",
    "        test[col]=le.transform(test[col])  \n",
    "    #----------------------------------------\n",
    "    train_data_y = np.array(train_data_y)\n",
    "    train_data_ret = np.array(train_data_ret)\n",
    "    #--------------------------------\n",
    "    #--------------------------------\n",
    "    \n",
    "    return train_data_y,train_data_ret,train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fullVisitorId', 'networkDomain', 'city', 'operatingSystem', 'metro',\n",
       "       'region', 'channelGrouping', 'country', 'source', 'medium', 'browser',\n",
       "       'deviceCategory', 'continent', 'pageviews_sum', 'pageviews_min',\n",
       "       'pageviews_max', 'pageviews_mean', 'hits_sum', 'hits_min', 'hits_max',\n",
       "       'hits_mean', 'visitStartTime_counts', 'isTrueDirect', 'newVisits_max',\n",
       "       'isMobile', 'visitNumber_max', 'transactionRevenue_sum',\n",
       "       'first_ses_from_the_period_start', 'last_ses_from_the_period_end',\n",
       "       'interval_dates', 'unqiue_date_num', 'target', 'ret', 'index', 'date',\n",
       "       'visitId', 'visitNumber', 'visitStartTime', 'device.browser',\n",
       "       'device.deviceCategory', 'device.isMobile', 'device.operatingSystem',\n",
       "       'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country',\n",
       "       'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region',\n",
       "       'geoNetwork.subContinent', 'totals.hits', 'totals.newVisits',\n",
       "       'totals.pageviews', 'totals.transactionRevenue',\n",
       "       'trafficSource.adContent', 'trafficSource.isTrueDirect',\n",
       "       'trafficSource.medium', 'trafficSource.source', '_weekday', '_day',\n",
       "       '_month', '_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train_data_y,train_data_ret,train,test=final_fun_1train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def final_fun_2(train_data_y,train_data_ret,train,test,fullVisitorId_new):\n",
    "    params_lgb2 = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"max_leaves\": 256,\n",
    "        \"num_leaves\" : 9,\n",
    "        \"min_child_samples\" : 1,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"bagging_fraction\" : 0.9,\n",
    "        \"feature_fraction\" : 0.8,\n",
    "        \"bagging_frequency\" : 1      \n",
    "    }\n",
    "    params_lgb1 = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"binary_logloss\",\n",
    "        \"max_leaves\": 256,\n",
    "        \"num_leaves\" : 15,\n",
    "        \"min_child_samples\" : 1,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"bagging_fraction\" : 0.9,\n",
    "        \"feature_fraction\" : 0.8,\n",
    "        \"bagging_frequency\" : 1           \n",
    "    }\n",
    "    #---------------------------------------------------\n",
    "    target_cols = ['target', 'ret', 'fullVisitorId',\"date\"]\n",
    "\n",
    "    dtrain = lgb.Dataset(train.drop(target_cols, axis=1), label=train['ret'])\n",
    "\n",
    "    dtrain_ret = lgb.Dataset(train.drop(target_cols, axis=1)[train['ret']==1], \n",
    "                         label=train['target'][train['ret']==1])\n",
    "    pr_lgb_sum = 0\n",
    "    #----------------------------------------------------\n",
    "    print('Training and predictions')\n",
    "    for i in range(10):\n",
    "        print('Interation number ', i)\n",
    "        lgb_model1 = lgb.train(params_lgb1, dtrain, num_boost_round=1200)\n",
    "        pr_lgb = lgb_model1.predict(test.drop(target_cols, axis=1))\n",
    "\n",
    "        lgb_model2 = lgb.train(params_lgb2, dtrain_ret, num_boost_round=368)\n",
    "        pr_lgb_ret = lgb_model2.predict(test.drop(target_cols, axis=1))\n",
    "\n",
    "        pr_lgb_sum = pr_lgb_sum + pr_lgb*pr_lgb_ret\n",
    "\n",
    "    pr_final_lgb = pr_lgb_sum/10\n",
    "    #-------------------------------\n",
    "    new_data=pd.DataFrame()\n",
    "    new_data[\"fullVisitorId\"]=fullVisitorId_new\n",
    "    new_data[\"PredictedLogRevenue\"]=pr_final_lgb\n",
    "    new_data.to_csv(path+\"new1_baseline.csv\", index=False)\n",
    "    return new_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predictions\n",
      "Interation number  0\n",
      "Interation number  1\n",
      "Interation number  2\n",
      "Interation number  3\n",
      "Interation number  4\n",
      "Interation number  5\n",
      "Interation number  6\n",
      "Interation number  7\n",
      "Interation number  8\n",
      "Interation number  9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th>PredictedLogRevenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1708337</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1708338</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1708339</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1708340</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1708341</td>\n",
       "      <td>0.000972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fullVisitorId  PredictedLogRevenue\n",
       "0        1708337             0.000972\n",
       "1        1708338             0.001002\n",
       "2        1708339             0.000972\n",
       "3        1708340             0.001002\n",
       "4        1708341             0.000972"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=final_fun_2(train_data_y,train_data_ret,train,test,fullVisitorId_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
